{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark Streaming\n",
    "\n",
    "This little demo uses Spark DStreams to ingest words from a stream, determine how long the words are, and then plots the distribution of string lengths over time. You can try feeding in different books from, say, Project Gutenberg to see the distribution change with books from different periods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: see `streamer.sh` for an example of a small program that streams out individual words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.streaming import StreamingContext\n",
    "# The \"1\" here is the number of seconds between microbatches:\n",
    "ssc = StreamingContext(sc, 1)\n",
    "\n",
    "# Required to be able to do state updates:\n",
    "ssc.checkpoint(\"checkpoint\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assumes the stream is running on the same machine as the driver.\n",
    "# That's not very common, so you'll probably change 'localhost'\n",
    "# to something else. In fact, using 'localhost' even from the local\n",
    "# machine seems to be hit or miss.\n",
    "sock = ssc.socketTextStream(\"localhost\", 8888)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Updates our distribution\n",
    "def update_dist(new_values, old_values):\n",
    "    return sum(new_values) + (old_values or 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plots the distribution. This executes on the driver!\n",
    "def plot_distribution(rdd):\n",
    "    df = pd.DataFrame(rdd.collect(), columns=['word size', 'frequency'])\n",
    "    df.sort_values('word size', ascending=True).plot(kind='bar', x='word size')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get word lengths and emit (len, 1) pairs\n",
    "lengths = sock.map(lambda word: len(word))\n",
    "counts = lengths.map(lambda length: (length, 1))\n",
    "\n",
    "# Count the instances of the lengths, then\n",
    "# add them to our stored state\n",
    "reduced = counts.reduceByKey(lambda x, y: x + y)\n",
    "distrib = reduced.updateStateByKey(update_dist)\n",
    "\n",
    "# Plot the new distribution for each microbatch\n",
    "distrib.foreachRDD(plot_distribution)\n",
    "\n",
    "# Print out the distribution for good measure\n",
    "distrib.pprint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running this will start listening:\n",
    "ssc.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTANT: you need the stopSparkContext=False, otherwise\n",
    "# your driver will die and you'll have to restart Jupyter\n",
    "ssc.stop(stopSparkContext=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
