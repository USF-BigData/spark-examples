{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1e5d5be",
   "metadata": {},
   "source": [
    "# Loading the Data\n",
    "\n",
    "First, we'll load some data to play with. Spark can load a variety of formats:\n",
    "\n",
    "```\n",
    "spark.read.text()\n",
    "spark.read.json()\n",
    "spark.read.format(<format goes here, e.g., csv>)\n",
    "```\n",
    "\n",
    "By default, it will attempt to automatically set up the schema if we are using dataframes / datasets.\n",
    "\n",
    "**On a somewhat related note**, if you've just stored a lot of data in HDFS to use with Spark, then you might want to rebalance your cluster with:\n",
    "\n",
    "```\n",
    "hdfs balancer -threshold 1\n",
    "```\n",
    "\n",
    "This should ensure that blocks are spread evenly across the workers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f3e86f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.json('hdfs://orion11:35001/RC_2008-12.bz2')\n",
    "#df = spark.read.json('hdfs://orion11:35001/RES-RC_2018-01.zst')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0dc3a3b",
   "metadata": {},
   "source": [
    "(let's wait for a bit while it loads our dataframe... Depending on how large the file is, it may take a while.)\n",
    "\n",
    "When this finally finishes, we can look at the data, and more importantly, our schema:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9528f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.take(2))\n",
    "print('There are {} records in this dataframe.'.format(df.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "935ae5a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "447200f0",
   "metadata": {},
   "source": [
    "# Defining a Schema \n",
    "\n",
    "We can actually define a schema ahead of time, assuming it won't change. Let's say we have a schema:\n",
    "\n",
    "```\n",
    "schema = df.schema\n",
    "```\n",
    "\n",
    "Then we can use\n",
    "\n",
    "```\n",
    "spark.load.schema(schema).json('...')\n",
    "```\n",
    "\n",
    "To avoid all the overhead from generating the schema automatically. This can be a big speed boost.\n",
    "\n",
    "However, be careful: in most datasets, the schema will change over time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77561549",
   "metadata": {},
   "source": [
    "# Playing with the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "499a5507",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see if any scores are over 9000\n",
    "df.filter(df.score > 9000).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41c481fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many posts were there in the politics subreddit?\n",
    "df.filter(df.subreddit == \"politics\").count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b1b87e5",
   "metadata": {},
   "source": [
    "# SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "053a4265",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView(\"df_view\")\n",
    "\n",
    "# How many different users posted in /r/politics this month?\n",
    "spark.sql(\"SELECT distinct author FROM df_view WHERE subreddit = 'politics'\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90ec2e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"{} unique users posted {} times!\".format(Out[11], Out[8]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb66a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What is the highest score (+/- upvotes/downvotes) in /r/technology?\n",
    "spark.sql(\"SELECT MAX(score) as max_score FROM df_view WHERE subreddit = 'technology'\").collect()[0][0]\n",
    "# NOTE: we get a 'row' result here, and index into it. It's safe to use .collect() here to transfer the data\n",
    "# to the driver, but if there was more than one row we have to be very cautious!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdab6d85",
   "metadata": {},
   "source": [
    "# Working with processed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e36edf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "\n",
    "top_authors = df.groupBy(\"author\").count().sort(col(\"count\").desc())\n",
    "top_authors.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4d10518",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "p = top_authors.limit(6).toPandas()\n",
    "p = p.iloc[1: , :] # Drop the first row"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "994bf7e8",
   "metadata": {},
   "source": [
    "Notice how we use 'limit' instead of take; limit returns a dataframe whereas 'take' is a terminal operation. Then we convert the limited dataframe to a Pandas dataframe.\n",
    "\n",
    "**Note:** if you want to view some data, `.show()` tends to be nice (see above)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bbbc810",
   "metadata": {},
   "outputs": [],
   "source": [
    "p.plot.barh()\n",
    "p"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
