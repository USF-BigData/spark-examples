{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.load(\n",
    "    'hdfs://orion12:9001/nam/2019/11/namanl_218_20191127*.grb2.tdv.gz',\n",
    "    format='csv',\n",
    "    sep='\\t',\n",
    "    inferSchema=True,\n",
    "    header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I loaded a smaller dataset than usual because training models can take a *LOT* of time. In practice, you want as much data as you can get your hands on, but we don't want to sit and watch Spark run for half the class.\n",
    "\n",
    "Let's take a look at our schema..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks good... (well, we probably don't need that last column -- _c18. It's blank because our .tdv files' header end with a tab character. I removed it in the sampled dataset).\n",
    "\n",
    "However, what would you need to do if you **didn't** have a workable schema autogenerated for you?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Schema Example (don't run on our NAM dataset!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# NOTE: this is just an example. It's not needed for our dataset!\n",
    "\n",
    "feats = []\n",
    "f = open('features.txt')\n",
    "for line_num, line in enumerate(f):\n",
    "    if line_num == 0:\n",
    "        # The first field is a long integer\n",
    "        feats.append(StructField(line.strip(), LongType(), True))\n",
    "    elif line_num == 1:\n",
    "        # The second field is a string\n",
    "        feats.append(StructField(line.strip(), StringType(), True))\n",
    "    else:\n",
    "        # All the other features are floats\n",
    "        feats.append(StructField(line.strip(), FloatType(), True))\n",
    "\n",
    "# I hard-coded some of the data types above. You could list them all out or use a loop if you have\n",
    "# several columns all with the same type.\n",
    "\n",
    "schema = StructType(feats)\n",
    "print(schema)\n",
    "\n",
    "df = spark.read.format('csv').option('sep', '\\t').schema(schema).load('hdfs://path/to/your/custom/dataset')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## And now... back to Machine Learning!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "def prepare_data(dframe, predictors, target):\n",
    "    assembler = VectorAssembler(inputCols=predictors, outputCol=\"features\")\n",
    "    output = assembler.transform(dframe)\n",
    "    return output.select(\"features\", target).withColumnRenamed(target, \"label\")\n",
    "\n",
    "# Choose our dependent and independent variables:\n",
    "prepped = prepare_data(df,\n",
    "    [\"precipitable_water_entire_atmosphere_single_layer\", \n",
    "     \"pressure_surface\",\n",
    "         \"relative_humidity_zerodegc_isotherm\", \n",
    "         \"snow_depth_surface\", \n",
    "         \"albedo_surface\"],\n",
    "    \"temperature_surface\")\n",
    "\n",
    "prepped.show()\n",
    "(trainingData, testData) = prepped.randomSplit([0.9, 0.1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "rf = RandomForestRegressor(numTrees=100, maxDepth=5, maxBins=32)\n",
    "model = rf.fit(trainingData)\n",
    "predictions = model.transform(testData)\n",
    "\n",
    "evaluator = RegressionEvaluator(\n",
    "    labelCol=\"label\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(\"Root Mean Squared Error (RMSE) on test data = %g\" % rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I got an RMSE of 3.82 K, which is not particularly great... We also need to look at the lag plot for this. Pandas does a decent job of doing this automatically, but we can also customize the plot with matplotlib directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib notebook\n",
    "\n",
    "p_df = predictions.select(\"label\", \"prediction\").toPandas()\n",
    "\n",
    "plt.suptitle('Random Forest Regressor', fontsize=16)\n",
    "\n",
    "minval = p_df[['label', 'prediction']].min().min()\n",
    "maxval = p_df[['label', 'prediction']].max().max()\n",
    "plt.axis([minval, maxval, minval, maxval])\n",
    "\n",
    "plt.plot(p_df['label'], p_df['prediction'], '.', color='#2ba5f1')\n",
    "plt.plot(range(int(minval), int(maxval)), range(int(minval), int(maxval)), lw=3, dashes=(10, 3), color='#000000', alpha=0.25, label='Ideal Predictor')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
